import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc
import matplotlib.pyplot as plt
from sklearn.metrics import PrecisionRecallDisplay
from sklearn.neural_network import MLPClassifier

data = pd.read_excel('SA_peptide_ver3.xlsx')
X = data.drop(columns=['MIC', 'MIC_64'])
Y = data['MIC_64']

#data = pd.read_excel('antifungal_dataset_simple.xlsx')
#X = data.drop(columns=['MIC_64'])
#Y = data['MIC_64']
print(X)
print(Y)

# Standardize data
sc = StandardScaler()
X_scaled = sc.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns = X.columns, index = X.index.values.tolist())
# Split data
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.25, random_state=0, stratify= Y)

# Define classifiers and their parameters
classifiers = {
    'RandomForestClassifier_recall': {
        'model': RandomForestClassifier(class_weight='balanced', max_depth=19, max_features='log2', min_samples_leaf=2, criterion='log_loss',
                       min_samples_split=4, n_estimators=311, random_state=0, bootstrap=True),
        'X_train': X_train,
        'X_test': X_test
    },
    'Random Forest Classifier_F1': {
        'model': RandomForestClassifier(class_weight='balanced', max_depth=15, max_features='log2', min_samples_leaf=2, criterion='entropy',
                       min_samples_split=4, n_estimators=300, random_state=0, bootstrap=True),
        'X_train': X_train,
        'X_test': X_test
    },
    'DecisionTreeClassifier_recall':{
        'model': DecisionTreeClassifier(class_weight='balanced', criterion='log_loss',
                       max_depth=22, max_features='sqrt', min_samples_leaf=14,
                       random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'DecisionTreeClassifier_f1':{
        'model': DecisionTreeClassifier(class_weight='balanced', criterion='entropy',
                       max_depth=14, max_features='sqrt', min_samples_leaf=4,
                       random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'SVC_recall': {
        'model': SVC(C=33.9550517513235, class_weight='balanced', gamma=0.02289882140927003,
    probability=True, random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'SVC_f1': {
        'model': SVC(kernel='rbf', gamma=0.028404348935883183, class_weight='balanced', C=39.91025275908208, random_state=0, probability=True),
        'X_train': X_train,
        'X_test': X_test
    },
    'MLP_recall': {
        'model':MLPClassifier(activation='logistic', alpha=0.02600602379764957,
              hidden_layer_sizes=141, learning_rate='adaptive', random_state=0,
              solver='sgd'),
        'X_train': X_train,
        'X_test': X_test
    },
    'MLP_f1': {
        'model':MLPClassifier(hidden_layer_sizes=147, learning_rate='constant', max_iter=371, solver='adam', activation='tanh', alpha=0.08906798781342089, random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'LogisticRegression_recall': {
        'model': LogisticRegression(C=23.116414536584802, class_weight='balanced', max_iter=27, random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'LogisticRegression_f1': {
        'model': LogisticRegression(random_state=0, max_iter=9, class_weight='balanced', C=55.464254318490546),
        'X_train': X_train,
        'X_test': X_test
    },
    'GaussianNB_recall': {
        'model': GaussianNB(var_smoothing=0.00033429878169282567),
        'X_train': X_train,
        'X_test': X_test
    },
    'GaussianNB_f1': {
        'model': GaussianNB(var_smoothing=0.00034549081183035847),
        'X_train': X_train,
        'X_test': X_test
    },
    'KNeighborsClassifier_recall': {
        'model': KNeighborsClassifier(algorithm='kd_tree', leaf_size=47, n_neighbors=47, p=1, weights='distance'),
        'X_train': X_train,
        'X_test': X_test
    },
    'KNeighborsClassifier_f1': {
        'model': KNeighborsClassifier(weights='uniform', p=2, n_neighbors=9, leaf_size=41, algorithm='auto'),
        'X_train': X_train,
        'X_test': X_test
    },
}

# Train and evaluate classifiers
for clf_name, clf_data in classifiers.items():
    model = clf_data['model']
    model.fit(clf_data['X_train'], Y_train.values.ravel())
    Y_pred = model.predict(clf_data['X_test'])
    cm = confusion_matrix(Y_test, Y_pred)
    TN, FP, FN, TP = cm.ravel()
    print("True Positive: ", TP)
    print("True Negative: ", TN)
    print("False Positive: ", FP)
    print("False Negative: ", FN)
    report = classification_report(Y_test, Y_pred, target_names=["Bad", "Good"])
    print(f"{clf_name} Classification Report:")
    print(report)
    # Calculate precision-recall curve and AUC
    if hasattr(model, 'predict_proba'):
        y_prob = model.predict_proba(clf_data['X_test'])[:, 1]
        precision, recall, _ = precision_recall_curve(Y_test, y_prob)
        pr_auc = auc(recall, precision)
        plt.rcParams['font.family'] = 'Times New Roman'
        # Plot precision-recall curve
        plt.figure()
        plt.plot(recall, precision, marker='o', label=f'{clf_name} (PR AUC = {pr_auc:.2f})')
        plt.xlabel("Precision (Positive label: 1)", fontsize=12)
        plt.ylabel("Recall (Positive label: 1)", fontsize=12)
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        plt.legend(fontsize=12)
        plt.show()
        display = PrecisionRecallDisplay.from_estimator(model, X_test, Y_test, name=f"{clf_name}")
        _ = display.ax_.set_title("")
        _ = display.ax_.set_ylabel("Precision (Positive label: 1)", fontsize=12)
        _ = display.ax_.set_xlabel("Recall (Positive label: 1)", fontsize=12)
        plt.legend(fontsize=12)
        plt.xticks(fontsize=12)
        plt.yticks(fontsize=12)
        print(model)
        print(f"PR AUC: {pr_auc}")

        plt.show()

