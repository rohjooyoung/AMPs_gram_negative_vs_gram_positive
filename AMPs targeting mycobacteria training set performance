import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
from sklearn.metrics import PrecisionRecallDisplay
from sklearn.neural_network import MLPClassifier

data = pd.read_excel('Mycobacteria_AMP_dataset.xlsx')
X = data.drop(columns=['MIC', 'MIC_64', 'MIC_32'])
Y = data['MIC_64']

# 5-fold cross validation
print(X)
print(Y)

# Standardize data
sc = StandardScaler()
X_scaled = sc.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns = X.columns, index = X.index.values.tolist())
# Split data
X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.25, random_state=0, stratify= Y)

# Define classifiers and their parameters
classifiers = {
    'RandomForestClassifier_recall': {
        'model': RandomForestClassifier(class_weight='balanced', criterion='entropy', max_depth=10, max_features='sqrt', min_samples_leaf=2, min_samples_split=5, n_estimators=57, bootstrap=True, random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'DecisionTreeClassifier_recall':{
        'model': DecisionTreeClassifier(random_state=0, min_samples_leaf=3, max_features='log2', max_depth=27, criterion='entropy', class_weight='balanced',  splitter='best'),
        'X_train': X_train,
        'X_test': X_test
    },
    'SVC_recall': {
        'model': SVC(kernel='rbf', gamma=0.7587050262057607, class_weight='balanced', C=71.37256691715945, random_state=0, probability=True),
        'X_train': X_train,
        'X_test': X_test
    },
    'MLP_recall': {
        'model': MLPClassifier(activation='logistic', alpha=0.055235137906738625, hidden_layer_sizes=112, learning_rate='adaptive', max_iter=437, random_state=0, solver='sgd'),
        'X_train': X_train,
        'X_test': X_test
    },
    'LogisticRegression_recall': {
        'model': LogisticRegression(random_state=0, max_iter=88, class_weight='balanced', C=22.896007244407233),
        'X_train': X_train,
        'X_test': X_test
    },
    'GaussianNB_recall': {
        'model': GaussianNB(var_smoothing=0.0009633176661125813),
        'X_train': X_train,
        'X_test': X_test
    },
    'KNeighborsClassifier_recall': {
        'model': KNeighborsClassifier(weights='distance', p=1, n_neighbors=4, leaf_size=34, algorithm='auto'),
        'X_train': X_train,
        'X_test': X_test
    },
    'RandomForestClassifier_f1': {
        'model': RandomForestClassifier(class_weight='balanced', criterion='entropy', max_depth=19, max_features='sqrt',
                                        min_samples_leaf=9, min_samples_split=6, n_estimators=29, bootstrap=True,
                                        random_state=0),
        'X_train': X_train,
        'X_test': X_test
    },
    'DecisionTreeClassifier_f1': {
        'model': DecisionTreeClassifier(random_state=0, min_samples_leaf=2, max_features='log2', max_depth=24,
                                        criterion='gini', class_weight='balanced', splitter='best'),
        'X_train': X_train,
        'X_test': X_test
    },
    'SVC_f1': {
        'model': SVC(kernel='poly', gamma=0.051062638693503484, class_weight='balanced', C=22.47608666946083,
                     random_state=0, probability=True),
        'X_train': X_train,
        'X_test': X_test
    },
    'MLP_f1': {
        'model': MLPClassifier(activation='logistic', alpha=0.08243775083360846,
                               hidden_layer_sizes=60, learning_rate='constant', max_iter=271, random_state=0,
                               solver='lbfgs'),
        'X_train': X_train,
        'X_test': X_test
    },
    'LogisticRegression_f1': {
        'model': LogisticRegression(random_state=0, max_iter=272, class_weight='balanced', C=0.33583842323318946),
        'X_train': X_train,
        'X_test': X_test
    },
    'GaussianNB_f1': {
        'model': GaussianNB(var_smoothing=0.00015204739205871616),
        'X_train': X_train,
        'X_test': X_test
    },
    'KNeighborsClassifier_f1': {
        'model': KNeighborsClassifier(weights='distance', p=2, n_neighbors=3, leaf_size=5, algorithm='auto'),
        'X_train': X_train,
        'X_test': X_test
    },
}

# Train and evaluate classifiers
for clf_name, clf_data in classifiers.items():
    model = clf_data['model']
    model.fit(clf_data['X_train'], Y_train.values.ravel())
    Y_pred = model.predict(clf_data['X_train'])
    cm = confusion_matrix(Y_train, Y_pred)
    TN, FP, FN, TP = cm.ravel()
    print("True Positive: ", TP)
    print("True Negative: ", TN)
    print("False Positive: ", FP)
    print("False Negative: ", FN)
    report = classification_report(Y_train, Y_pred, target_names=["Bad", "Good"])
    print(f"{clf_name} Classification Report:")
    print(report)
